{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group1 is:  3903 , Group2 is:  3979\n"
     ]
    }
   ],
   "source": [
    "# from matplotlib import pyplot\n",
    "import os\n",
    "\n",
    "#define a function to count the document number \n",
    "def Count(dirname):\n",
    "  count=0\n",
    "  for item in os.listdir(dirname):\n",
    "    count+=1\n",
    "  return count\n",
    "\n",
    "counter=[]\n",
    "\n",
    "#plot the histogram for 20 topics\n",
    "for dir in os.listdir('20news-bydate-train'):\n",
    "  dir_name='20news-bydate-train/'+dir\n",
    "  count1=Count(dir_name)\n",
    "  counter.append(count1)\n",
    "\n",
    "i=0;\n",
    "\n",
    "for dir in os.listdir('20news-bydate-test'):\n",
    "  #print counter[i],'\\n'\n",
    "  dir_name='20news-bydate-test/'+dir\n",
    "  count2=Count(dir_name)\n",
    "  counter[i]+=count2\n",
    "  #print counter[i],'\\n'\n",
    "  i+=1\n",
    "  \n",
    "#count the document number for the two groups\n",
    "new_counter=[0,0]\n",
    "for dir in os.listdir('20news-bydate-train'):\n",
    "  if(dir=='comp.graphics' or dir=='comp.os.ms-windows.misc' or dir=='comp.sys.ibm.pc.hardware' or dir=='comp.sys.mac.hardware'):\n",
    "    dir_name='20news-bydate-train/'+dir\n",
    "    count1=Count(dir_name)\n",
    "    new_counter[0]+=count1\n",
    "    #print (new_counter[0],'\\n')\n",
    "  if(dir=='rec.autos' or dir=='rec.motorcycles' or dir=='rec.sport.baseball' or dir=='rec.sport.hockey'):\n",
    "    dir_name='20news-bydate-train/'+dir\n",
    "    count1=Count(dir_name)\n",
    "    new_counter[1]+=count1\n",
    "    #print (new_counter[1],'\\n')\n",
    "    \n",
    "for dir in os.listdir('20news-bydate-test'):\n",
    "  if(dir=='comp.graphics' or dir=='comp.os.ms-windows.misc' or dir=='comp.sys.ibm.pc.hardware' or dir=='comp.sys.mac.hardware'):\n",
    "    dir_name='20news-bydate-test/'+dir\n",
    "    count2=Count(dir_name)\n",
    "    new_counter[0]+=count2\n",
    "    #print (new_counter[0],'\\n')\n",
    "  if(dir=='rec.autos' or dir=='rec.motorcycles' or dir=='rec.sport.baseball' or dir=='rec.sport.hockey'):\n",
    "    dir_name='20news-bydate-test/'+dir\n",
    "    count2=Count(dir_name)\n",
    "    new_counter[1]+=count2\n",
    "    #print (new_counter[1],'\\n')\n",
    "\n",
    "print ('Group1 is: ',new_counter[0],', Group2 is: ',new_counter[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ziyin\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:519: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "n = 20\n",
    "values = (counter[0],counter[1],counter[2],counter[3],counter[4],counter[5],counter[6],counter[7],counter[8],counter[9],counter[10],counter[11],counter[12],counter[13],counter[14],counter[15],counter[16],counter[17],counter[18],counter[19])\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n)\n",
    "bar_width = 0.35\n",
    "opacity = 1.0\n",
    "bars = plt.bar(index, values, bar_width,alpha = opacity, color = 'b')\n",
    "plt.xlabel('Topics')\n",
    "plt.xlabel('Documents')\n",
    "plt.title('Number of documents per topic')\n",
    "plt.xticks(index + bar_width, ('1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'))\n",
    "plt.ylim(0,1000)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18846, 72399)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "import re\n",
    "import nltk \n",
    "import string\n",
    "#nltk.download()\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenizer_fun(text):\n",
    "    new_text = re.sub(r'[^A-Za-z]', \" \", text)\n",
    "    tokens =[word for sent in nltk.sent_tokenize(new_text) for word in nltk.word_tokenize(sent)]\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]{2,}', token):\n",
    "            new_tokens.append(token)     \n",
    "    stem = [stemmer.stem(t) for t in new_tokens]\n",
    "    return stem\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "all_data = fetch_20newsgroups(subset='all', shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TFxIDF = TfidfVectorizer(analyzer='word',tokenizer=tokenizer_fun, stop_words=stop_words,token_pattern='[a-zA-Z]{2,}',)\n",
    "TFxIDF_data = TFxIDF.fit_transform(all_data.data)\n",
    "count = TFxIDF_data.shape\n",
    "\n",
    "print (count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['mb', 'ani', 'disk', 'work', 'problem', 'drive', 'card', 'use', 'control', 'scsi'])\n",
      "dict_keys(['drive', 'know', 'mac', 'problem', 'like', 'appl', 'monitor', 'work', 'use', 'ani'])\n",
      "dict_keys(['includ', 'sale', 'offer', 'drive', 'sell', 'ship', 'price', 'new', 'use', 'pleas'])\n",
      "dict_keys(['church', 'think', 'peopl', 'god', 'jesus', 'say', 'christian', 'sin', 'know', 'believ'])\n"
     ]
    }
   ],
   "source": [
    "list=['comp.sys.ibm.pc.hardware','comp.sys.mac.hardware','misc.forsale','soc.religion.christian']\n",
    "\n",
    "for category in list:\n",
    "    categories = [category]\n",
    "    sub_data = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "    TFxIDF_sub = TfidfVectorizer(analyzer='word',tokenizer=tokenizer_fun, max_features=10, stop_words=stop_words,token_pattern='[a-zA-Z]{2,}')\n",
    "    sub_count = TFxIDF_sub.fit_transform(sub_data.data)\n",
    "    #print (sub_count)\n",
    "    print (TFxIDF_sub.vocabulary_.keys())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Problem(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "X = TruncatedSVD(n_components=50)\n",
    "LSI = X.fit_transform(TFxIDF_data)\n",
    "#print (LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Problem(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions to get data or LSI for train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = ['comp.graphics','comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware','comp.sys.ibm.pc.hardware','rec.autos','rec.motorcycles','rec.sport.baseball','rec.sport.hockey']\n",
    "\n",
    "def data_fun(train_or_test):\n",
    "    if(train_or_test == 'train'):\n",
    "        data = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "    if(train_or_test == 'test'):\n",
    "        data = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "    return data  \n",
    "\n",
    "def LSI_fun(train_or_test):\n",
    "    if(train_or_test == 'train'):\n",
    "        data = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "        TFxIDF_sub = TfidfVectorizer(analyzer='word',tokenizer=tokenizer_fun, stop_words=stop_words,token_pattern='[a-zA-Z]{2,}',)\n",
    "        TFxIDF_sub_data = TFxIDF_sub.fit_transform(data.data)\n",
    "        Y = TruncatedSVD(n_components=50)\n",
    "        sub_LSI = Y.fit_transform(TFxIDF_sub_data)\n",
    "    if(train_or_test == 'test'):\n",
    "        data = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "        TFxIDF_sub = TfidfVectorizer(analyzer='word',tokenizer=tokenizer_fun, stop_words=stop_words,token_pattern='[a-zA-Z]{2,}',)\n",
    "        TFxIDF_sub_data = TFxIDF_sub.fit_transform(data.data)\n",
    "        Y = TruncatedSVD(n_components=50)\n",
    "        sub_LSI = Y.fit_transform(TFxIDF_sub_data)\n",
    "    return sub_LSI \n",
    "\n",
    "#new_LSI = LSI_fun('train')\n",
    "#print (new_LSI.shape)\n",
    "\n",
    "#new_LSI = LSI_fun('test')\n",
    "#print (new_LSI.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 5 7 4 6 1 2 4 6 0 7 2 0 7 5 7 5 2 5 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "lin_clf = svm.LinearSVC()\n",
    "# print(all_data.target)\n",
    "train_LSI = LSI_fun('train')\n",
    "train_set = data_fun('train')\n",
    "\n",
    "train_target_group = [ int(x / 4) for x in train_set.target]\n",
    "# print(train_target_group)\n",
    "print(train_set.target[0:20])\n",
    "# lin_clf.fit(train_LSI, train_set.target)\n",
    "lin_clf.fit(train_LSI, train_target_group)\n",
    "#lin_clf.fit(train_LSI, train_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2765, 50)\n",
      "[1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "test_LSI = LSI_fun('test')\n",
    "print(test_LSI.shape)\n",
    "svm_predicted = lin_clf.predict(test_LSI)\n",
    "print (svm_predicted[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM-ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "test_set = data_fun('test')\n",
    "test_target_group = [ int(x / 4) for x in test_set.target]\n",
    "y_score_test = lin_clf.decision_function(test_LSI)\n",
    "\n",
    "\n",
    "#print(test_target_group[0:10])\n",
    "#print(y_score_test.shape)\n",
    "#print(y_score_test[0:10])\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_target_group, y_score_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC fold (area = %0.2f)' % roc_auc)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM-Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 962,  213],\n",
       "       [ 456, 1134]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_target_group, svm_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM-Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.758047016275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "svm_accuracy = accuracy_score(test_target_group, svm_predicted)\n",
    "print (svm_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.841870824053\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "svm_precision_score = precision_score(test_target_group, svm_predicted)\n",
    "print (svm_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71320754717\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "svm_recall_score = recall_score(test_target_group, svm_predicted)\n",
    "print (svm_recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM-Recall and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5758783   0.57572464  0.57593331  0.57614213  0.57635111  0.57656023\n",
      "  0.57676951  0.57697894  0.57718852  0.57739826]\n",
      "[ 1.          0.99937107  0.99937107  0.99937107  0.99937107  0.99937107\n",
      "  0.99937107  0.99937107  0.99937107  0.99937107]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "precision, recall, _ = precision_recall_curve(test_target_group,y_score_test)\n",
    "average_precision = average_precision_score(test_target_group, y_score_test)\n",
    "print (precision[0:10])\n",
    "print (recall[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2765,)\n",
      "[1 0 1 0 0 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(train_LSI, train_target_group)\n",
    "nb_predicted = nb_clf.predict(test_LSI)\n",
    "print (nb_predicted.shape)\n",
    "print (nb_predicted[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB-ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2765,)\n",
      "[  9.99981017e-01   2.21490497e-05   9.99995834e-01   2.81952490e-20\n",
      "   4.75679624e-03   1.10226832e-50   9.99951101e-01   1.58551558e-06\n",
      "   3.41865903e-01   9.99985624e-01]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "test_set = data_fun('test')\n",
    "test_target_group = [ int(x / 4) for x in test_set.target]\n",
    "y_score_test_nb = nb_clf.predict_proba(test_LSI)[:,1]\n",
    "\n",
    "\n",
    "print(y_score_test_nb.shape)\n",
    "print(y_score_test_nb[0:10])\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_target_group, y_score_test_nb)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC fold (area = %0.2f)' % roc_auc)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB-Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[929, 246],\n",
       "       [742, 848]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(test_target_group, nb_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB-Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.642676311031\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "nb_accuracy = accuracy_score(test_target_group, nb_predicted)\n",
    "print (nb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.775137111517\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "nb_precision_score = precision_score(test_target_group, nb_predicted)\n",
    "print (nb_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.533333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "nb_recall_score = recall_score(test_target_group, nb_predicted)\n",
    "print (nb_recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB-Recall and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.57504521  0.69976771  0.70151869  0.70211516  0.70271868  0.70219065\n",
      "  0.70243035  0.70284698  0.70326409  0.70434265]\n",
      "[ 1.          0.75786164  0.75534591  0.75157233  0.74779874  0.74591195\n",
      "  0.74528302  0.74528302  0.74528302  0.74465409]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "y_score_test_nb = nb_clf.predict_proba(test_LSI)[:,1]\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "precision, recall, _ = precision_recall_curve(test_target_group,y_score_test_nb)\n",
    "average_precision = average_precision_score(test_target_group, y_score_test_nb)\n",
    "print (precision[0:10])\n",
    "print (recall[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### LR classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2765,)\n",
      "[1 0 1 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model, datasets\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "logreg.fit(train_LSI, train_target_group)\n",
    "lr_predicted = logreg.predict(test_LSI)\n",
    "print (lr_predicted.shape)\n",
    "print (lr_predicted[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR-ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2765,)\n",
      "[  0.99116574  -1.77739445   0.49283749 -13.4189053    6.31897121\n",
      "   8.96282951  -0.0274014    2.17361211  -5.2876063    1.90199889]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "test_set = data_fun('test')\n",
    "test_target_group = [ int(x / 4) for x in test_set.target]\n",
    "y_score_test_lr = logreg.decision_function(test_LSI)\n",
    "#y_score_test_lr = logreg.predict_proba(test_LSI)[:,1]\n",
    "\n",
    "print(y_score_test_lr.shape)\n",
    "print(y_score_test_lr[0:10])\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_target_group, y_score_test_lr)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC fold (area = %0.2f)' % roc_auc)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR-Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 933,  242],\n",
       "       [ 534, 1056]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(test_target_group, lr_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR-Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.719349005425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "lr_accuracy = accuracy_score(test_target_group, lr_predicted)\n",
    "print (lr_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.813559322034\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "lr_precision_score = precision_score(test_target_group, lr_predicted)\n",
    "print (lr_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.664150943396\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "lr_recall_score = recall_score(test_target_group, lr_predicted)\n",
    "print (lr_recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR-Recall and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.57650471  0.57635111  0.57656023  0.57676951  0.57697894  0.57718852\n",
      "  0.57739826  0.57760814  0.57781818  0.57802837]\n",
      "[ 1.          0.99937107  0.99937107  0.99937107  0.99937107  0.99937107\n",
      "  0.99937107  0.99937107  0.99937107  0.99937107]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "y_score_test_lr = logreg.decision_function(test_LSI)\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "precision, recall, _ = precision_recall_curve(test_target_group,y_score_test_lr)\n",
    "average_precision = average_precision_score(test_target_group, y_score_test_lr)\n",
    "print (precision[0:10])\n",
    "print (recall[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions to get data or LSI for train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories_i = ['comp.sys.ibm.pc.hardware','comp.sys.mac.hardware','misc.forsale','soc.religion.christian']\n",
    "\n",
    "def data_fun_i(train_or_test):\n",
    "    if(train_or_test == 'train'):\n",
    "        data = fetch_20newsgroups(subset='train', categories=categories_i, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "    if(train_or_test == 'test'):\n",
    "        data = fetch_20newsgroups(subset='test', categories=categories_i, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "    return data\n",
    "\n",
    "def LSI_fun_i(train_or_test):\n",
    "    if(train_or_test == 'train'):\n",
    "        data = fetch_20newsgroups(subset='train', categories=categories_i, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "        TFxIDF_sub = TfidfVectorizer(analyzer='word',tokenizer=tokenizer_fun, stop_words=stop_words,token_pattern='[a-zA-Z]{2,}',)\n",
    "        TFxIDF_sub_data = TFxIDF_sub.fit_transform(data.data)\n",
    "        Y = TruncatedSVD(n_components=50)\n",
    "        sub_LSI = Y.fit_transform(TFxIDF_sub_data)\n",
    "    if(train_or_test == 'test'):\n",
    "        data = fetch_20newsgroups(subset='test', categories=categories_i, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "        TFxIDF_sub = TfidfVectorizer(analyzer='word',tokenizer=tokenizer_fun, stop_words=stop_words,token_pattern='[a-zA-Z]{2,}',)\n",
    "        TFxIDF_sub_data = TFxIDF_sub.fit_transform(data.data)\n",
    "        Y = TruncatedSVD(n_components=50)\n",
    "        sub_LSI = Y.fit_transform(TFxIDF_sub_data)\n",
    "    return sub_LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier \n",
    "from sklearn.multiclass import OneVsOneClassifier \n",
    "from sklearn.svm import LinearSVC\n",
    "train_LSI_i = LSI_fun_i('train')\n",
    "train_set_i = data_fun_i('train')\n",
    "test_LSI_i = LSI_fun_i('test')\n",
    "test_set_i = data_fun_i('test')\n",
    "#train_target_group_i = [ int(x / 10) for x in train_set_i.target]\n",
    "#test_target_group_i = [int(x / 10) for x in test_set_i.target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one vs rest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 ..., 0 2 3]\n"
     ]
    }
   ],
   "source": [
    "ovr_classifier_i = OneVsRestClassifier (LinearSVC())\n",
    "ovr_classifier_i.fit(train_LSI_i, train_set_i.target)\n",
    "ovr_predict_i = ovr_classifier_i.predict(test_LSI_i)\n",
    "#print (ovr_predict_i[0:4])\n",
    "print (ovr_predict_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OVR-Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[111, 217,  55,   9],\n",
       "       [127, 113, 130,  15],\n",
       "       [ 34,  47, 302,   7],\n",
       "       [  0,   3,  22, 373]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_set_i.target, ovr_predict_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OVR-Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.574440894569\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ovr_accuracy = accuracy_score(test_set_i.target, ovr_predict_i)\n",
    "print (ovr_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.558027296054\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "ovr_precision_score = precision_score(test_set_i.target, ovr_predict_i,average='weighted')\n",
    "print (ovr_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.574440894569\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "ovr_recall_score = recall_score(test_set_i.target, ovr_predict_i,average='weighted')\n",
    "print (ovr_recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one vs one classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1565,)\n",
      "[2 0 2 ..., 0 2 3]\n"
     ]
    }
   ],
   "source": [
    "ovo_classifier_i = OneVsOneClassifier (LinearSVC())\n",
    "ovo_classifier_i.fit(train_LSI_i, train_set_i.target)\n",
    "ovo_predict_i = ovo_classifier_i.predict(test_LSI_i)\n",
    "print (ovo_predict_i.shape)\n",
    "print (ovo_predict_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OVO-Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[120, 212,  60,   0],\n",
       "       [135, 113, 133,   4],\n",
       "       [ 34,  45, 307,   4],\n",
       "       [  1,  10,  30, 357]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_set_i.target, ovo_predict_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OVO-Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.573162939297\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ovo_accuracy = accuracy_score(test_set_i.target, ovo_predict_i)\n",
    "print (ovo_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56988888905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "ovo_precision_score = precision_score(test_set_i.target, ovo_predict_i,average='weighted')\n",
    "print (ovo_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.573162939297\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "ovo_recall_score = recall_score(test_set_i.target, ovo_predict_i,average='weighted')\n",
    "print (ovo_recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB Multiclass Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1565,)\n",
      "[2 0 3 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_classifier_i = GaussianNB()\n",
    "nb_classifier_i.fit(train_LSI_i, train_set_i.target)\n",
    "nb_predict_i = nb_classifier_i.predict(test_LSI_i)\n",
    "print (nb_predict_i.shape)\n",
    "print (nb_predict_i[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB-Multi Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[138, 164,  64,  26],\n",
       "       [109, 129,  97,  50],\n",
       "       [ 72,  51, 204,  63],\n",
       "       [ 98,  19,  52, 229]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_set_i.target, nb_predict_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB-Multi-Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.447284345048\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "nb_accuracy = accuracy_score(test_set_i.target, nb_predict_i)\n",
    "print (nb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.450482201719\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "nb_precision_score = precision_score(test_set_i.target, nb_predict_i,average='weighted')\n",
    "print (nb_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.447284345048\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "nb_recall_score = recall_score(test_set_i.target, nb_predict_i,average='weighted')\n",
    "print (nb_recall_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
