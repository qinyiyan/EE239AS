{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group1 is:  3903 , Group2 is:  3979\n"
     ]
    }
   ],
   "source": [
    "# from matplotlib import pyplot\n",
    "import os\n",
    "\n",
    "#define a function to count the document number \n",
    "def Count(dirname):\n",
    "  count=0\n",
    "  for item in os.listdir(dirname):\n",
    "    count+=1\n",
    "  return count\n",
    "\n",
    "counter=[]\n",
    "\n",
    "#plot the histogram for 20 topics\n",
    "for dir in os.listdir('20news-bydate-train'):\n",
    "  dir_name='20news-bydate-train/'+dir\n",
    "  count1=Count(dir_name)\n",
    "  counter.append(count1)\n",
    "\n",
    "i=0;\n",
    "\n",
    "for dir in os.listdir('20news-bydate-test'):\n",
    "  #print counter[i],'\\n'\n",
    "  dir_name='20news-bydate-test/'+dir\n",
    "  count2=Count(dir_name)\n",
    "  counter[i]+=count2\n",
    "  #print counter[i],'\\n'\n",
    "  i+=1\n",
    "  \n",
    "#count the document number for the two groups\n",
    "new_counter=[0,0]\n",
    "for dir in os.listdir('20news-bydate-train'):\n",
    "  if(dir=='comp.graphics' or dir=='comp.os.ms-windows.misc' or dir=='comp.sys.ibm.pc.hardware' or dir=='comp.sys.mac.hardware'):\n",
    "    dir_name='20news-bydate-train/'+dir\n",
    "    count1=Count(dir_name)\n",
    "    new_counter[0]+=count1\n",
    "    #print (new_counter[0],'\\n')\n",
    "  if(dir=='rec.autos' or dir=='rec.motorcycles' or dir=='rec.sport.baseball' or dir=='rec.sport.hockey'):\n",
    "    dir_name='20news-bydate-train/'+dir\n",
    "    count1=Count(dir_name)\n",
    "    new_counter[1]+=count1\n",
    "    #print (new_counter[1],'\\n')\n",
    "    \n",
    "for dir in os.listdir('20news-bydate-test'):\n",
    "  if(dir=='comp.graphics' or dir=='comp.os.ms-windows.misc' or dir=='comp.sys.ibm.pc.hardware' or dir=='comp.sys.mac.hardware'):\n",
    "    dir_name='20news-bydate-test/'+dir\n",
    "    count2=Count(dir_name)\n",
    "    new_counter[0]+=count2\n",
    "    #print (new_counter[0],'\\n')\n",
    "  if(dir=='rec.autos' or dir=='rec.motorcycles' or dir=='rec.sport.baseball' or dir=='rec.sport.hockey'):\n",
    "    dir_name='20news-bydate-test/'+dir\n",
    "    count2=Count(dir_name)\n",
    "    new_counter[1]+=count2\n",
    "    #print (new_counter[1],'\\n')\n",
    "\n",
    "print ('Group1 is: ',new_counter[0],', Group2 is: ',new_counter[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ziyin\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:519: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "n = 20\n",
    "values = (counter[0],counter[1],counter[2],counter[3],counter[4],counter[5],counter[6],counter[7],counter[8],counter[9],counter[10],counter[11],counter[12],counter[13],counter[14],counter[15],counter[16],counter[17],counter[18],counter[19])\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n)\n",
    "bar_width = 0.35\n",
    "opacity = 1.0\n",
    "bars = plt.bar(index, values, bar_width,alpha = opacity, color = 'b')\n",
    "plt.xlabel('Topics')\n",
    "plt.xlabel('\\nDocuments')\n",
    "plt.title('Number of documents per topic')\n",
    "plt.xticks(index + bar_width, ('atheism', 'graphics', 'windows.\\nmisc', 'pc.\\nhardware', 'mac.\\nhardware', 'windows.x', 'forsale', 'autos', 'motorcycles', 'sport.\\nbaseball', 'sport.\\nhockey', 'crypt', 'electronics', 'med', 'space', 'christian', 'guns', 'mideast', 'politics.\\nmisc', 'religion.\\nmisc'))\n",
    "plt.ylim(0,1000)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18846, 72399)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "import re\n",
    "import nltk \n",
    "import string\n",
    "#nltk.download()\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenizer_fun(text):\n",
    "    new_text = re.sub(r'[^A-Za-z]', \" \", text)\n",
    "    tokens =[word for sent in nltk.sent_tokenize(new_text) for word in nltk.word_tokenize(sent)]\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]{2,}', token):\n",
    "            new_tokens.append(token)     \n",
    "    stem = [stemmer.stem(t) for t in new_tokens]\n",
    "    return stem\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "all_data = fetch_20newsgroups(subset='all', shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TFxIDF = TfidfVectorizer(analyzer='word',tokenizer=tokenizer_fun, stop_words=stop_words,token_pattern='[a-zA-Z]{2,}',)\n",
    "TFxIDF_data = TFxIDF.fit_transform(all_data.data)\n",
    "count = TFxIDF_data.shape\n",
    "\n",
    "print (count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['control', 'work', 'problem', 'disk', 'drive', 'ani', 'mb', 'scsi', 'use', 'card'])\n",
      "dict_keys(['work', 'problem', 'drive', 'ani', 'like', 'monitor', 'appl', 'know', 'mac', 'use'])\n",
      "dict_keys(['offer', 'ship', 'new', 'includ', 'drive', 'sale', 'sell', 'pleas', 'price', 'use'])\n",
      "dict_keys(['sin', 'believ', 'god', 'know', 'peopl', 'jesus', 'christian', 'think', 'say', 'church'])\n"
     ]
    }
   ],
   "source": [
    "my_list=['comp.sys.ibm.pc.hardware','comp.sys.mac.hardware','misc.forsale','soc.religion.christian']\n",
    "\n",
    "for category in my_list:\n",
    "    categories = [category]\n",
    "    sub_data = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "    TFxIDF_sub = TfidfVectorizer(analyzer='word',tokenizer=tokenizer_fun, max_features=10, stop_words=stop_words,token_pattern='[a-zA-Z]{2,}')\n",
    "    sub_count = TFxIDF_sub.fit_transform(sub_data.data)\n",
    "    #print (sub_count)\n",
    "    print (TFxIDF_sub.vocabulary_.keys())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Problem(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "X = TruncatedSVD(n_components=50, algorithm='arpack')\n",
    "LSI = X.fit_transform(TFxIDF_data)\n",
    "#print (LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Problem(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions to get data or LSI for train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = ['comp.graphics','comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware','comp.sys.mac.hardware','rec.autos','rec.motorcycles','rec.sport.baseball','rec.sport.hockey']\n",
    "\n",
    "def data_fun(train_or_test):\n",
    "    if(train_or_test == 'train'):\n",
    "        data = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "    if(train_or_test == 'test'):\n",
    "        data = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "    return data  \n",
    "\n",
    "def LSI_fun(train_or_test):\n",
    "    if(train_or_test == 'train'):\n",
    "        data = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "        TFxIDF_sub = TfidfVectorizer(tokenizer=tokenizer_fun, stop_words=stop_words,token_pattern='[a-zA-Z]{2,}',)\n",
    "        TFxIDF_sub_data = TFxIDF_sub.fit_transform(data.data)\n",
    "        Y = TruncatedSVD(n_components=50, algorithm='arpack')\n",
    "        sub_LSI = Y.fit_transform(TFxIDF_sub_data)\n",
    "    if(train_or_test == 'test'):\n",
    "        data = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "        TFxIDF_sub = TfidfVectorizer(tokenizer=tokenizer_fun, stop_words=stop_words,token_pattern='[a-zA-Z]{2,}',)\n",
    "        TFxIDF_sub_data = TFxIDF_sub.fit_transform(data.data)\n",
    "        Y = TruncatedSVD(n_components=50, algorithm='arpack')\n",
    "        sub_LSI = Y.fit_transform(TFxIDF_sub_data)\n",
    "    return sub_LSI \n",
    "\n",
    "#new_LSI = LSI_fun('train')\n",
    "#print (new_LSI.shape)\n",
    "\n",
    "#new_LSI = LSI_fun('test')\n",
    "#print (new_LSI.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "lin_clf = LinearSVC()\n",
    "# print(all_data.target)\n",
    "train_LSI = LSI_fun('train')\n",
    "train_set = data_fun('train')\n",
    "\n",
    "train_target_group = [ int(x / 4) for x in train_set.target]\n",
    "# print(train_target_group)\n",
    "#print(train_set.target[0:20])\n",
    "# lin_clf.fit(train_LSI, train_set.target)\n",
    "lin_clf.fit(train_LSI, train_target_group)\n",
    "#lin_clf.fit(train_LSI, train_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3150, 50)\n",
      "[1 0 0 0 0 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "test_LSI = LSI_fun('test')\n",
    "print(test_LSI.shape)\n",
    "svm_predicted = lin_clf.predict(test_LSI)\n",
    "print (svm_predicted[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM-ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "test_set = data_fun('test')\n",
    "test_target_group = [ int(x / 4) for x in test_set.target]\n",
    "y_score_test = lin_clf.decision_function(test_LSI)\n",
    "\n",
    "#print(test_target_group[0:10])\n",
    "#print(y_score_test.shape)\n",
    "#print(y_score_test[0:10])\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_target_group, y_score_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC fold (area = %0.2f)' % roc_auc)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC_SVM Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM-Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1322,  238],\n",
       "       [ 393, 1197]])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_target_group, svm_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM-Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799682539683\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "svm_accuracy = accuracy_score(test_target_group, svm_predicted)\n",
    "print (svm_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834146341463\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "svm_precision_score = precision_score(test_target_group, svm_predicted)\n",
    "print (svm_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.752830188679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "svm_recall_score = recall_score(test_target_group, svm_predicted)\n",
    "print (svm_recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM-Recall and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "precision, recall, _ = precision_recall_curve(test_target_group,y_score_test)\n",
    "average_precision = average_precision_score(test_target_group, y_score_test)\n",
    "#print (precision[0:10])\n",
    "#print (recall[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best parameter lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.cross_validation.KFold(n=4732, n_folds=5, shuffle=False, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "kf = KFold(len(train_target_group), n_folds=5,shuffle=False,random_state=None)\n",
    "len(kf)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "#initialize a matrix with 7 rows(lamda) and 5 columns(5 validation)\n",
    "scores = [[0]*7 for i in range(5)]\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    X_train, X_test = train_LSI[train_index], train_LSI[test_index]\n",
    "    for k in [-3, -2, -1, 0, 1, 2, 3]:\n",
    "        X_train_target_group = [ int(x / 4) for x in train_set.target[train_index]]\n",
    "        X_test_target_group = [ int(x / 4) for x in train_set.target[test_index]]\n",
    "        \n",
    "        soft_svm_clf = svm.LinearSVC(C=10**k)\n",
    "        soft_svm_clf.fit(X_train, X_train_target_group)\n",
    "        scores[i][j]= soft_svm_clf.score(X_test, X_test_target_group)        \n",
    "        j=j+1\n",
    "    i=i+1\n",
    "    j=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86962701844703771, 0.9171604555165862, 0.93005016397614815, 0.93913683134232717, 0.94082749352020723, 0.94125211249053975, 0.93047790842786049]\n",
      "0.941252112491\n",
      "The best lambda is 100\n"
     ]
    }
   ],
   "source": [
    "ave_score = list(map(lambda x: (x[0]+x[1]+x[2]+x[3]+x[4])/5, zip(scores[0], scores[1], scores[2], scores[3], scores[4])))\n",
    "max_score = max(ave_score)\n",
    "print (ave_score)\n",
    "print (max_score)\n",
    "index=ave_score.index(max_score)\n",
    "r = [-3, -2, -1, 0, 1, 2, 3]\n",
    "print ('The best lambda is',10**r[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft_SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soft_clf = svm.LinearSVC(C=10**-2)\n",
    "soft_clf.fit(train_LSI, train_target_group)\n",
    "soft_svm_predicted = soft_clf.predict(test_LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft_SVM-ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "test_set = data_fun('test')\n",
    "test_target_group = [ int(x / 4) for x in test_set.target]\n",
    "y_score_test_soft_svm = soft_clf.decision_function(test_LSI)\n",
    "\n",
    "#print(test_target_group[0:10])\n",
    "#print(y_score_test.shape)\n",
    "#print(y_score_test[0:10])\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_target_group, y_score_test_soft_svm)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC fold (area = %0.2f)' % roc_auc)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC_SVM_soft Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft_SVM Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1331,  229],\n",
       "       [ 228, 1362]])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(test_target_group, soft_svm_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft_clf-Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854920634921\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "soft_clf_accuracy = accuracy_score(test_target_group, soft_svm_predicted)\n",
    "print (soft_clf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856065367693\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "soft_clf_precision_score = precision_score(test_target_group, soft_svm_predicted)\n",
    "print (soft_clf_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856603773585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "soft_clf_recall_score = recall_score(test_target_group, soft_svm_predicted)\n",
    "print (soft_clf_recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian-NB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3150,)\n",
      "[0 0 0 0 0 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(train_LSI, train_target_group)\n",
    "nb_predicted = nb_clf.predict(test_LSI)\n",
    "print (nb_predicted.shape)\n",
    "print (nb_predicted[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian-NB-ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "test_set = data_fun('test')\n",
    "test_target_group = [ int(x / 4) for x in test_set.target]\n",
    "y_score_test_nb = nb_clf.predict_proba(test_LSI)[:,1]\n",
    "\n",
    "#print(y_score_test_nb.shape)\n",
    "#print(y_score_test_nb[0:10])\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_target_group, y_score_test_nb)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC fold (area = %0.2f)' % roc_auc)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC_NB Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian-NB-Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1257,  303],\n",
       "       [ 686,  904]])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(test_target_group, nb_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian-NB-Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.686031746032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "nb_accuracy = accuracy_score(test_target_group, nb_predicted)\n",
    "print (nb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.748964374482\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "nb_precision_score = precision_score(test_target_group, nb_predicted)\n",
    "print (nb_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.568553459119\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "nb_recall_score = recall_score(test_target_group, nb_predicted)\n",
    "print (nb_recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian-NB-Recall and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "y_score_test_nb = nb_clf.predict_proba(test_LSI)[:,1]\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "precision, recall, _ = precision_recall_curve(test_target_group,y_score_test_nb)\n",
    "average_precision = average_precision_score(test_target_group, y_score_test_nb)\n",
    "#print (precision[0:10])\n",
    "#print (recall[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bernoulli-NB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3150,)\n",
      "[0 0 0 0 0 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb_clf = BernoulliNB()\n",
    "nb_clf.fit(train_LSI, train_target_group)\n",
    "nb_predicted = nb_clf.predict(test_LSI)\n",
    "print (nb_predicted.shape)\n",
    "print (nb_predicted[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bernoulli-NB-ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "test_set = data_fun('test')\n",
    "test_target_group = [ int(x / 4) for x in test_set.target]\n",
    "y_score_test_nb = nb_clf.predict_proba(test_LSI)[:,1]\n",
    "\n",
    "#print(y_score_test_nb.shape)\n",
    "#print(y_score_test_nb[0:10])\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_target_group, y_score_test_nb)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC fold (area = %0.2f)' % roc_auc)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC_NB Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bernoulli-NB-Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1366,  194],\n",
       "       [ 202, 1388]])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(test_target_group, nb_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bernoulli-NB-Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.874285714286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "nb_accuracy = accuracy_score(test_target_group, nb_predicted)\n",
    "print (nb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.877370417193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "nb_precision_score = precision_score(test_target_group, nb_predicted)\n",
    "print (nb_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.872955974843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "nb_recall_score = recall_score(test_target_group, nb_predicted)\n",
    "print (nb_recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bernoulli-NB-Recall and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "y_score_test_nb = nb_clf.predict_proba(test_LSI)[:,1]\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "precision, recall, _ = precision_recall_curve(test_target_group,y_score_test_nb)\n",
    "average_precision = average_precision_score(test_target_group, y_score_test_nb)\n",
    "#print (precision[0:10])\n",
    "#print (recall[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### LR classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3150,)\n",
      "[1 0 0 1 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model, datasets\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "logreg.fit(train_LSI, train_target_group)\n",
    "lr_predicted = logreg.predict(test_LSI)\n",
    "print (lr_predicted.shape)\n",
    "print (lr_predicted[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR-ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "test_set = data_fun('test')\n",
    "test_target_group = [ int(x / 4) for x in test_set.target]\n",
    "y_score_test_lr = logreg.decision_function(test_LSI)\n",
    "#y_score_test_lr = logreg.predict_proba(test_LSI)[:,1]\n",
    "\n",
    "#print(y_score_test_lr.shape)\n",
    "#print(y_score_test_lr[0:10])\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_target_group, y_score_test_lr)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC fold (area = %0.2f)' % roc_auc)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC_LR Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR-Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1270,  290],\n",
       "       [ 466, 1124]])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(test_target_group, lr_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR-Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "lr_accuracy = accuracy_score(test_target_group, lr_predicted)\n",
    "print (lr_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.794908062235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "lr_precision_score = precision_score(test_target_group, lr_predicted)\n",
    "print (lr_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.706918238994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "lr_recall_score = recall_score(test_target_group, lr_predicted)\n",
    "print (lr_recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR-Recall and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "y_score_test_lr = logreg.decision_function(test_LSI)\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "precision, recall, _ = precision_recall_curve(test_target_group,y_score_test_lr)\n",
    "average_precision = average_precision_score(test_target_group, y_score_test_lr)\n",
    "#print (precision[0:10])\n",
    "#print (recall[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions to get data or LSI for train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories_i = ['comp.sys.ibm.pc.hardware','comp.sys.mac.hardware','misc.forsale','soc.religion.christian']\n",
    "\n",
    "def data_fun_i(train_or_test):\n",
    "    if(train_or_test == 'train'):\n",
    "        data = fetch_20newsgroups(subset='train', categories=categories_i, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "    if(train_or_test == 'test'):\n",
    "        data = fetch_20newsgroups(subset='test', categories=categories_i, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "    return data\n",
    "\n",
    "def LSI_fun_i(train_or_test):\n",
    "    if(train_or_test == 'train'):\n",
    "        data = fetch_20newsgroups(subset='train', categories=categories_i, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "        TFxIDF_sub = TfidfVectorizer(analyzer='word',tokenizer=tokenizer_fun, stop_words=stop_words,token_pattern='[a-zA-Z]{2,}',)\n",
    "        TFxIDF_sub_data = TFxIDF_sub.fit_transform(data.data)\n",
    "        Y = TruncatedSVD(n_components=50, algorithm='arpack')\n",
    "        sub_LSI = Y.fit_transform(TFxIDF_sub_data)\n",
    "    if(train_or_test == 'test'):\n",
    "        data = fetch_20newsgroups(subset='test', categories=categories_i, shuffle=True, random_state=42, remove=('headers','footers','quotes'))\n",
    "        TFxIDF_sub = TfidfVectorizer(analyzer='word',tokenizer=tokenizer_fun, stop_words=stop_words,token_pattern='[a-zA-Z]{2,}',)\n",
    "        TFxIDF_sub_data = TFxIDF_sub.fit_transform(data.data)\n",
    "        Y = TruncatedSVD(n_components=50, algorithm='arpack')\n",
    "        sub_LSI = Y.fit_transform(TFxIDF_sub_data)\n",
    "    return sub_LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier \n",
    "from sklearn.multiclass import OneVsOneClassifier \n",
    "from sklearn.svm import LinearSVC\n",
    "train_LSI_i = LSI_fun_i('train')\n",
    "train_set_i = data_fun_i('train')\n",
    "test_LSI_i = LSI_fun_i('test')\n",
    "test_set_i = data_fun_i('test')\n",
    "#train_target_group_i = [ int(x / 10) for x in train_set_i.target]\n",
    "#test_target_group_i = [int(x / 10) for x in test_set_i.target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one vs rest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 ..., 0 2 3]\n"
     ]
    }
   ],
   "source": [
    "ovr_classifier_i = OneVsRestClassifier (LinearSVC())\n",
    "ovr_classifier_i.fit(train_LSI_i, train_set_i.target)\n",
    "ovr_predict_i = ovr_classifier_i.predict(test_LSI_i)\n",
    "#print (ovr_predict_i[0:4])\n",
    "print (ovr_predict_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OVR-Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[149, 178,  60,   5],\n",
       "       [124,  94, 140,  27],\n",
       "       [ 37,  35, 308,  10],\n",
       "       [  4,   5,  28, 361]])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_set_i.target, ovr_predict_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OVR-Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.582747603834\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ovr_accuracy = accuracy_score(test_set_i.target, ovr_predict_i)\n",
    "print (ovr_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.563982043464\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "ovr_precision_score = precision_score(test_set_i.target, ovr_predict_i,average='weighted')\n",
    "print (ovr_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.582747603834\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "ovr_recall_score = recall_score(test_set_i.target, ovr_predict_i,average='weighted')\n",
    "print (ovr_recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one vs one classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1565,)\n",
      "[2 0 2 ..., 0 2 3]\n"
     ]
    }
   ],
   "source": [
    "ovo_classifier_i = OneVsOneClassifier (LinearSVC())\n",
    "ovo_classifier_i.fit(train_LSI_i, train_set_i.target)\n",
    "ovo_predict_i = ovo_classifier_i.predict(test_LSI_i)\n",
    "print (ovo_predict_i.shape)\n",
    "print (ovo_predict_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OVO-Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[156, 175,  59,   2],\n",
       "       [145,  94, 136,  10],\n",
       "       [ 37,  37, 310,   6],\n",
       "       [  5,  10,  35, 348]])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_set_i.target, ovo_predict_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OVO-Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.580191693291\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ovo_accuracy = accuracy_score(test_set_i.target, ovo_predict_i)\n",
    "print (ovo_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.571965590504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "ovo_precision_score = precision_score(test_set_i.target, ovo_predict_i,average='weighted')\n",
    "print (ovo_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.580191693291\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "ovo_recall_score = recall_score(test_set_i.target, ovo_predict_i,average='weighted')\n",
    "print (ovo_recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian NB Multiclass Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1565,)\n",
      "[2 0 3 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_classifier_i = GaussianNB()\n",
    "nb_classifier_i.fit(train_LSI_i, train_set_i.target)\n",
    "nb_predict_i = nb_classifier_i.predict(test_LSI_i)\n",
    "print (nb_predict_i.shape)\n",
    "print (nb_predict_i[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian-NB-Multi Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[137, 161,  72,  22],\n",
       "       [109, 127, 105,  44],\n",
       "       [ 79,  45, 204,  62],\n",
       "       [ 97,  18,  56, 227]])"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_set_i.target, nb_predict_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian-NB-Multi-Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.444089456869\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "nb_accuracy = accuracy_score(test_set_i.target, nb_predict_i)\n",
    "print (nb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.449276631965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "nb_precision_score = precision_score(test_set_i.target, nb_predict_i,average='weighted')\n",
    "print (nb_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.444089456869\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "nb_recall_score = recall_score(test_set_i.target, nb_predict_i,average='weighted')\n",
    "print (nb_recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bernoulli NB Multiclass Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1565,)\n",
      "[2 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb_classifier_i = BernoulliNB()\n",
    "nb_classifier_i.fit(train_LSI_i, train_set_i.target)\n",
    "nb_predict_i = nb_classifier_i.predict(test_LSI_i)\n",
    "print (nb_predict_i.shape)\n",
    "print (nb_predict_i[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Bernoulli-NB-Multi Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[163, 168,  54,   7],\n",
       "       [200,  86,  80,  19],\n",
       "       [ 63,  22, 293,  12],\n",
       "       [ 18,  26,  47, 307]])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_set_i.target, nb_predict_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bernoulli-NB-Multi-Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54249201278\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "nb_accuracy = accuracy_score(test_set_i.target, nb_predict_i)\n",
    "print (nb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.542353955084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "nb_precision_score = precision_score(test_set_i.target, nb_predict_i,average='weighted')\n",
    "print (nb_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54249201278\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "nb_recall_score = recall_score(test_set_i.target, nb_predict_i,average='weighted')\n",
    "print (nb_recall_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
