{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#gohawks\n",
      "#gopatriots\n",
      "#nfl\n",
      "#patriots\n",
      "#sb49\n",
      "#superbowl\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "from __future__ import division\n",
    "from __future__ import with_statement\n",
    "\n",
    "\n",
    "# convert the month tag in the timestamp into number \n",
    "def month_convert(time):\n",
    "    for i in range (len(time)):\n",
    "        if 'Dec' in time[i]:\n",
    "            time[i]= time[i].replace('Dec','12')\n",
    "        elif 'Jan' in time[i]:\n",
    "            time[i]=time[i].replace('Jan','01')\n",
    "        elif 'Feb' in time[i]:\n",
    "            time[i]=time[i].replace('Feb','02')\n",
    "    return time\n",
    "\n",
    "\n",
    "# apply time window (1h) for the data to extract features\n",
    "def apply_time_window(tweet_num, feature):\n",
    "    feature_each_hour=[]\n",
    "    for num in tweet_num:\n",
    "        if len(feature)==0:\n",
    "            break\n",
    "        elif num==0:       \n",
    "            feature_each_hour.append(0)\n",
    "        else:\n",
    "            num_each=0\n",
    "            for i in range(num):\n",
    "                if len(feature)==0:\n",
    "                    break\n",
    "                else:\n",
    "                    num_each=num_each+feature.pop(0)\n",
    "            feature_each_hour.append(num_each)\n",
    "    return feature_each_hour\n",
    "\n",
    "\n",
    "# plot number of tweets versus feature value and save corresponding figure\t\n",
    "def customed_plot(feature, tweets_num, hashtag, feature_name):\n",
    "    plt.plot(feature, tweets_num,'bo')\n",
    "    plt.xlabel(feature_name + ' value in each 1-hour window')\n",
    "    plt.ylabel('number of tweets in each 1-hour window')\n",
    "    plt.savefig('tweets_vs_' + feature_name + '_' + hashtag)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def feature_exploration(hashtag):\n",
    "\n",
    "    title=[]\n",
    "    post_date=[]\n",
    "    follower=[]\n",
    "    citations=[]\n",
    "    acceleration=[]\n",
    "    favorite=[]\n",
    "    influential=[]\n",
    "    matching=[]\n",
    "    momentum=[]\n",
    "    ranking=[]\n",
    "    statuses=[]\n",
    "    user_metion=[]\n",
    "    emotion=[]\n",
    " \n",
    "    # get each original feature data from file \n",
    "    with open(\"tweets_\"+hashtag+\".txt\") as f:\n",
    "        for line in f:\n",
    "            result=json.loads(line)\n",
    "            title.append(result[\"title\"])\n",
    "            post_date.append(result[\"tweet\"][\"created_at\"])\n",
    "            follower.append(result[\"author\"][\"followers\"])\n",
    "            citations.append(result[\"metrics\"][\"citations\"][\"total\"])\n",
    "            acceleration.append(result[\"metrics\"][\"acceleration\"])\n",
    "            favorite.append(result[\"tweet\"][\"favorite_count\"])\n",
    "            influential.append(result[\"metrics\"][\"citations\"][\"influential\"])\n",
    "            matching.append(result[\"metrics\"][\"citations\"][\"matching\"])\n",
    "            momentum.append(result[\"metrics\"][\"momentum\"])\n",
    "            ranking.append(result[\"metrics\"][\"ranking_score\"])\n",
    "            statuses.append(result[\"tweet\"][\"user\"][\"statuses_count\"])\n",
    "\n",
    "    for metion in title:\n",
    "        user_metion.append(metion.count(\"@\"))\n",
    "  \n",
    "    for content in title:\n",
    "        if (content.find(\"!!!\")>0 or content.find(\"???\")>0):\n",
    "            emotion.append(1)\n",
    "        else:\n",
    "            emotion.append(0)\n",
    "\n",
    "    follower_copy=[]\n",
    "    follower_copy=follower_copy+follower\n",
    "\n",
    "    time=[]\n",
    "    for time_stamp in post_date:\n",
    "        time.append(json.dumps(time_stamp[-4:]+time_stamp[-5]+time_stamp[4:19]))\n",
    "    time=month_convert(time)\n",
    "\n",
    "    # create a list which contains the start and end of each time window (1h)\n",
    "    start_date_all = datetime.datetime(int(time[0][1:5]),int(time[0][6:8]),int(time[0][9:11]), int(time[0][12:14]),00,0)\n",
    "    end_date_all = datetime.datetime(int(time[-1][1:5]),int(time[-1][6:8]),int(time[-1][9:11]), int(time[-1][12:14])+1,00,0)\n",
    "    delta = datetime.timedelta(hours=1)\n",
    "    time_interval=[]\n",
    "    while start_date_all < end_date_all:\n",
    "        end_date = start_date_all + delta\n",
    "        time_interval.append(end_date.strftime(\"%Y %m %d %H:%M:%S\"))\n",
    "        start_date_all += delta\n",
    "\n",
    "    time_of_the_day=[]\n",
    "    for time_stamp in time_interval:\n",
    "        time_of_the_day.append(time_stamp[11:13])\n",
    "\n",
    "    number_each_hour=[]   \n",
    "    for time_limit in time_interval:\n",
    "        count=0\n",
    "        for time_stamp in time:\n",
    "            count+=1\n",
    "            if time_stamp[1:-1]>time_limit:\n",
    "                number_each_hour.append(count)\n",
    "                break\n",
    "\n",
    "\n",
    "    # get the number of tweets in each 1h time window\n",
    "    tweet_num=[]\n",
    "    for i in range (len(number_each_hour)):\n",
    "        number_each_hour[i]=number_each_hour[i]-1\n",
    "\n",
    "    tweet_num.append(number_each_hour[0])\n",
    "    for i in range (len(number_each_hour)-1):\n",
    "        tweet_num.append(number_each_hour[i+1]-number_each_hour[i])   \n",
    "    tweet_num.append(count-number_each_hour[-1])\n",
    "\n",
    "\n",
    "    # create time window (1h) from the data to extract features\n",
    "    # each window will provide a sample for the following regression model trainning \n",
    "    retweet_each_hour=apply_time_window(tweet_num, citations)\n",
    "    follower_each_hour=apply_time_window(tweet_num, follower)\n",
    "    acceleration_each_hour=apply_time_window(tweet_num, acceleration)\n",
    "    favorite_each_hour=apply_time_window(tweet_num, favorite)\n",
    "    influential_each_hour=apply_time_window(tweet_num, influential)\n",
    "    matching_each_hour=apply_time_window(tweet_num, matching)  \n",
    "    momentum_each_hour=apply_time_window(tweet_num, momentum)\n",
    "    ranking_each_hour=apply_time_window(tweet_num, ranking)\t\n",
    "    statuses_each_hour=apply_time_window(tweet_num, statuses)\n",
    "    emotion_each_hour=apply_time_window(tweet_num, emotion)\n",
    "    usermetion_each_hour=apply_time_window(tweet_num, user_metion)\n",
    "    \n",
    "    # caculate maximum number of followers in each hour seperately, since the method is a little different\n",
    "    max_follower_each_hour=[]\n",
    "    for num in tweet_num:\n",
    "        if len(follower_copy)==0:\n",
    "            break\n",
    "        elif num==0:\n",
    "            max_follower_each_hour.append(0)\n",
    "        else:\n",
    "            num_each=[]\n",
    "            for i in range(num):\n",
    "                if len(follower_copy)==0:\n",
    "                    break\n",
    "                else:\n",
    "                    num_each.append(follower_copy.pop(0))\n",
    "            max_follower_each_hour.append(max(num_each))\n",
    "\n",
    "\n",
    "    # save the processed data which can be used for trainning model in a csv file for each hashtag       \n",
    "    csv_file = open(\"Processed_Data_\"+hashtag+\".csv\", 'wb')\n",
    "    csv_write = csv.writer(csv_file) \n",
    "    csv_write.writerow(tweet_num)\n",
    "    csv_write.writerow(retweet_each_hour)\n",
    "    csv_write.writerow(follower_each_hour)\n",
    "    csv_write.writerow(max_follower_each_hour)\n",
    "    csv_write.writerow(time_of_the_day)   \n",
    "    csv_write.writerow(acceleration_each_hour)\n",
    "    csv_write.writerow(favorite_each_hour)\n",
    "    csv_write.writerow(influential_each_hour)\n",
    "    csv_write.writerow(matching_each_hour)\n",
    "    csv_write.writerow(momentum_each_hour)\n",
    "    csv_write.writerow(ranking_each_hour)\n",
    "    csv_write.writerow(statuses_each_hour)\n",
    "    csv_write.writerow(usermetion_each_hour)\n",
    "    csv_write.writerow(emotion_each_hour)\n",
    "    \n",
    "\n",
    "    # plot number of tweets versus each feature value\n",
    "    customed_plot(retweet_each_hour,tweet_num,hashtag,'retweet')\n",
    "    customed_plot(follower_each_hour,tweet_num,hashtag,'follower')\n",
    "    customed_plot(max_follower_each_hour,tweet_num,hashtag,'max_follower') \n",
    "    customed_plot(time_of_the_day,tweet_num,hashtag,'time_of_the_day')\n",
    "    customed_plot(acceleration_each_hour,tweet_num,hashtag,'acceleration')\n",
    "    customed_plot(favorite_each_hour,tweet_num,hashtag,'favorite')\n",
    "    customed_plot(influential_each_hour,tweet_num,hashtag,'influential')\n",
    "    customed_plot(matching_each_hour,tweet_num,hashtag,'matching')\n",
    "    customed_plot(momentum_each_hour,tweet_num,hashtag,'momentum')\n",
    "    customed_plot(ranking_each_hour,tweet_num,hashtag,'ranking')\n",
    "    customed_plot(statuses_each_hour,tweet_num,hashtag,'statuses')\n",
    "    customed_plot(usermetion_each_hour,tweet_num,hashtag,'usermetion')\n",
    "    customed_plot(emotion_each_hour,tweet_num,hashtag,'emotion')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hashtag_list=[\"#gohawks\", \"#gopatriots\", \"#nfl\", \"#patriots\", \"#sb49\", \"#superbowl\"]\n",
    "    for hashtag in hashtag_list:\n",
    "        print(hashtag)\n",
    "        feature_exploration(hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
